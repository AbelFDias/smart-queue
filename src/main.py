"""
SMART QUEUE - Sistema de GestÃ£o de Filas com VisÃ£o Computacional
DetecÃ§Ã£o de pessoas usando YOLOv8 local (sem necessidade de API/internet!)

Tecnologias:
- YOLOv8 (ultralytics) para detecÃ§Ã£o de pessoas
- OpenCV para captura e processamento de vÃ­deo
- YAML para configuraÃ§Ã£o

Autor: Abel Dias e SimÃ£o Marcos
"""

import cv2
import yaml
import time
from pathlib import Path
from ultralytics.models.yolo import YOLO

# ============================================
# CONFIGURAÃ‡ÃƒO
# ============================================

# DiretÃ³rio raiz do projeto
ROOT_DIR = Path(__file__).parent.parent

# Carregar configuraÃ§Ã£o do ficheiro YAML
config_path = ROOT_DIR / 'config' / 'config.yaml'
with open(config_path, 'r') as f:
    CONFIG = yaml.safe_load(f)

# Extrair configuraÃ§Ãµes
VIDEO_SOURCE = CONFIG.get('video_source', 0)
PROCESS_EVERY_N = CONFIG.get('process_every_n_frames', 3)
CONFIDENCE = CONFIG.get('confidence_threshold', 0.5)
YOLO_MODEL = CONFIG.get('yolo_model', 'yolov8n.pt')

# ParÃ¢metros (fÃ¡ceis de ajustar)
LINE_MATCH_MAX_DIST = 60  # px, raio para associar centroides entre frames
LINE_COLOR = (0, 0, 255)  # BGR (vermelho)
LINE_THICKNESS = 2

# Carregar modelo YOLO
# Na primeira execuÃ§Ã£o faz download automÃ¡tico (~6MB para nano)
print("ðŸ”„ A carregar modelo YOLO...")

# Resolver caminho do modelo (suporta caminho relativo Ã  raiz do projeto)
_model_cfg_path = Path(YOLO_MODEL)
_resolved_model_path = _model_cfg_path if _model_cfg_path.is_absolute() else (ROOT_DIR / _model_cfg_path)

if _resolved_model_path.exists():
    MODEL = YOLO(str(_resolved_model_path))
    print(f"âœ… Modelo carregado de: {_resolved_model_path}")
else:
    # Fallback: usar identificador do modelo (Ultralytics faz download se necessÃ¡rio)
    print(f"â„¹ï¸  Modelo local nÃ£o encontrado em '{_resolved_model_path}'. A tentar carregar '{YOLO_MODEL}'.")
    MODEL = YOLO(YOLO_MODEL)
    print("âœ… Modelo carregado com sucesso (Ultralytics)")

# ============================================
# FUNÃ‡Ã•ES
# ============================================

def detect_people(frame):
    """
    Detecta pessoas num frame usando YOLOv8 local.
    
    Args:
        frame: Frame numpy array (BGR) do OpenCV
    
    Returns:
        Lista de dicionÃ¡rios com detecÃ§Ãµes:
        [{
            'x1': int, 'y1': int,  # Canto superior esquerdo
            'x2': int, 'y2': int,  # Canto inferior direito
            'confidence': float    # ConfianÃ§a da detecÃ§Ã£o (0-1)
        }, ...]
    """
    # InferÃªncia YOLO
    # classes=[0] = apenas pessoas (classe 0 do COCO dataset)
    # verbose=False = sem prints no terminal
    results = MODEL(frame, conf=CONFIDENCE, classes=[0], verbose=False)
    
    # Extrair bounding boxes
    detections = []
    for result in results:
        boxes = result.boxes
        for box in boxes:
            # Coordenadas do bounding box
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            conf = float(box.conf[0])
            
            detections.append({
                'x1': int(x1),
                'y1': int(y1),
                'x2': int(x2),
                'y2': int(y2),
                'confidence': conf
            })
    
    return detections


def draw_detections(frame, detections):
    """
    Desenha bounding boxes sobre as detecÃ§Ãµes de pessoas.
    
    Args:
        frame: Frame onde desenhar
        detections: Lista de detecÃ§Ãµes da funÃ§Ã£o detect_people()
    
    Returns:
        Frame com as detecÃ§Ãµes desenhadas
    """
    for det in detections:
        x1, y1 = det['x1'], det['y1']
        x2, y2 = det['x2'], det['y2']
        conf = det['confidence']
        
        # Desenhar retÃ¢ngulo verde em volta da pessoa
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        
        # Label com confianÃ§a
        label = f"Pessoa: {conf:.0%}"
        
        # Fundo para o texto (melhor legibilidade)
        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
        label_w, label_h = label_size
        cv2.rectangle(frame, (x1, y1 - label_h - 10), (x1 + label_w, y1), (0, 255, 0), -1)
        
        # Texto
        cv2.putText(frame, label, (x1, y1 - 5),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
    
    return frame


def draw_info(frame, fps, num_people):
    """
    Desenha painel de informaÃ§Ãµes no canto superior esquerdo.
    
    Args:
        frame: Frame onde desenhar
        fps: FPS atual
        num_people: NÃºmero de pessoas detectadas
    
    Returns:
        Frame com overlay de informaÃ§Ãµes
    """
    # Criar fundo semi-transparente
    overlay = frame.copy()
    cv2.rectangle(overlay, (10, 10), (280, 85), (0, 0, 0), -1)
    cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)
    
    # InformaÃ§Ãµes
    cv2.putText(frame, f"FPS: {fps:.1f}", (20, 35),
               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
    cv2.putText(frame, f"Pessoas detectadas: {num_people}", (20, 60),
               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
    
    return frame


# ============================================
# LINE-CROSSING HELPERS
# ============================================

def _sign(x: float, eps: float = 1e-3) -> int:
    if x > eps:
        return 1
    if x < -eps:
        return -1
    return 0


def _point_side(p, a, b) -> float:
    # cross((b - a), (p - a))
    return (b[0] - a[0]) * (p[1] - a[1]) - (b[1] - a[1]) * (p[0] - a[0])


def _crossed_line(prev_p, curr_p, a, b) -> bool:
    s1 = _sign(_point_side(prev_p, a, b))
    s2 = _sign(_point_side(curr_p, a, b))
    return s1 != 0 and s2 != 0 and s1 != s2


# ============================================
# MAIN
# ============================================

def main():
    """Loop principal do sistema de detecÃ§Ã£o."""
    print("=" * 70)
    print("  ðŸŽ¯ SMART QUEUE - Sistema de GestÃ£o de Filas")
    print("  ðŸ“¹ DetecÃ§Ã£o local com YOLOv8 (sem necessidade de internet!)")
    print("=" * 70)
    print()
    
    # Abrir fonte de vÃ­deo
    print(f"ðŸ“¹ A abrir fonte de vÃ­deo: {VIDEO_SOURCE}")
    cap = cv2.VideoCapture(VIDEO_SOURCE)
    
    if not cap.isOpened():
        print(f"âŒ ERRO: NÃ£o foi possÃ­vel abrir a fonte de vÃ­deo: {VIDEO_SOURCE}")
        print("\nðŸ’¡ Dicas:")
        print("  - Verifica se a webcam estÃ¡ conectada")
        print("  - Tenta mudar video_source no config.yaml (0, 1, 2...)")
        print("  - Se usas Iriun, verifica se a app estÃ¡ a correr")
        return
    
    print(f"âœ… Fonte de vÃ­deo aberta com sucesso!")
    print()
    print("âš™ï¸  ConfiguraÃ§Ã£o:")
    print(f"  - Modelo: {YOLO_MODEL}")
    print(f"  - Processar: 1 em cada {PROCESS_EVERY_N} frames")
    print(f"  - ConfianÃ§a mÃ­nima: {CONFIDENCE:.0%}")
    print()
    print("ðŸŽ® Controlos:")
    print("  Q - Sair")
    print()
    print("=" * 70)
    print()
    
    # Estado
    frame_counter = 0
    last_detections = []
    fps = 0
    total_frames = 0
    start_time = time.time()
    # Estado para contagem por linha
    prev_centroids = []
    entry_count = 0
    # Linha vertical (fila esquerda â†’ direita), inicializa com base no tamanho do frame
    line_a = None  # (x, y)
    line_b = None  # (x, y)
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("âŒ Erro ao ler frame")
                break

            # Inicializar linha vertical apÃ³s obter dimensÃµes do frame
            if line_a is None:
                H, W = frame.shape[:2]
                x_mid = W // 2
                line_a = (x_mid, 0)
                line_b = (x_mid, H)
            
            total_frames += 1
            frame_counter += 1
            
            # Calcular FPS
            elapsed = time.time() - start_time
            if elapsed > 0:
                fps = total_frames / elapsed
            
            # Fazer detecÃ§Ã£o a cada N frames (para otimizar performance)
            if frame_counter >= PROCESS_EVERY_N:
                frame_counter = 0
                
                try:
                    last_detections = detect_people(frame)
                    num = len(last_detections)
                    print(f"ðŸ“Š [Frame {total_frames}] Detectadas {num} pessoa(s) | FPS: {fps:.1f}")

                    # Calcular centroides atuais
                    curr_centroids = [
                        ((d['x1'] + d['x2']) // 2, (d['y1'] + d['y2']) // 2)
                        for d in last_detections
                    ]

                    # Matching greedy com centroides do frame anterior
                    used_prev = set()
                    max_d2 = LINE_MATCH_MAX_DIST * LINE_MATCH_MAX_DIST
                    for c in curr_centroids:
                        best_i, best_d2 = None, 1e18
                        for i, p in enumerate(prev_centroids):
                            if i in used_prev:
                                continue
                            dx, dy = c[0] - p[0], c[1] - p[1]
                            d2 = dx * dx + dy * dy
                            if d2 < best_d2:
                                best_d2 = d2
                                best_i = i
                        if best_i is not None and best_d2 <= max_d2:
                            if _crossed_line(prev_centroids[best_i], c, line_a, line_b):
                                entry_count += 1
                            used_prev.add(best_i)

                    # Atualizar histÃ³rico
                    prev_centroids = curr_centroids
                except Exception as e:
                    print(f"âš ï¸  Erro na detecÃ§Ã£o: {e}")
                    last_detections = []
            
            # Desenhar
            if last_detections:
                frame = draw_detections(frame, last_detections)
            
            frame = draw_info(frame, fps, len(last_detections))

            # Desenhar linha de contagem (apÃ³s overlay para ficar visÃ­vel)
            if line_a is not None and line_b is not None:
                cv2.line(frame, line_a, line_b, LINE_COLOR, LINE_THICKNESS)

            # Mostrar total de entradas (no HUD)
            cv2.putText(frame, f"Entradas: {entry_count}", (20, 85),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            # Mostrar resultado
            cv2.imshow('Smart Queue - Sistema de DetecÃ§Ã£o', frame)
            
            # Verificar tecla pressionada
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q') or key == ord('Q'):
                print("\nðŸ›‘ A encerrar...")
                break
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interrompido pelo utilizador (Ctrl+C)")
    
    except Exception as e:
        print(f"\nâŒ Erro fatal: {e}")
    
    finally:
        # Libertar recursos
        cap.release()
        cv2.destroyAllWindows()
        
        # EstatÃ­sticas finais
        elapsed_time = time.time() - start_time
        print()
        print("=" * 70)
        print("ðŸ“Š EstatÃ­sticas da sessÃ£o:")
        print(f"  - Total de frames processados: {total_frames}")
        print(f"  - FPS mÃ©dio: {fps:.1f}")
        print(f"  - Tempo total: {elapsed_time:.1f}s")
        print("=" * 70)
        print("âœ… Sistema encerrado com sucesso!")


if __name__ == "__main__":
    main()
